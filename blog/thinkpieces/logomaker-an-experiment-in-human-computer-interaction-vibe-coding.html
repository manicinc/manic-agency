<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/5e3bf2e3cf44f85c.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/6f1e80598b39c648.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/e06744435527e09b.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/f1b33108a809bcd7.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/9922de4f935ecb48.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/573f7debcb2da6f3.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-97daa47057c0ad96.js"/><script src="/_next/static/chunks/4bd1b696-8ff4165fc225492c.js" async=""></script><script src="/_next/static/chunks/684-1c4fc75ccb0f6ff8.js" async=""></script><script src="/_next/static/chunks/main-app-331cb45b4a41ed8b.js" async=""></script><script src="/_next/static/chunks/9081a741-c64b490c58328843.js" async=""></script><script src="/_next/static/chunks/874-dce3bf739b01de3a.js" async=""></script><script src="/_next/static/chunks/830-9c5a0465a430e144.js" async=""></script><script src="/_next/static/chunks/app/layout-364d573b212e082c.js" async=""></script><script src="/_next/static/chunks/63-d95d5592655c601c.js" async=""></script><script src="/_next/static/chunks/741-a8214fa2992e7fbd.js" async=""></script><script src="/_next/static/chunks/app/not-found-7ced8bb09ca0d298.js" async=""></script><script src="/_next/static/chunks/932-0b16ec775ea3eed2.js" async=""></script><script src="/_next/static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-1a65ae41b9c69b5b.js" async=""></script><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><title>Logomaker: An experiment in human-computer interaction and ✨vibe coding ✨</title><meta name="description" content="Exploring the world of vibe coding through a logo creation tool, with insights on different LLMs and the changing landscape of software development."/><meta name="author" content="Johnny Dunn"/><meta property="og:title" content="Logomaker: An experiment in human-computer interaction and ✨vibe coding ✨"/><meta property="og:description" content="Exploring the world of vibe coding through a logo creation tool, with insights on different LLMs and the changing landscape of software development."/><meta property="og:type" content="article"/><meta property="article:published_time" content="2025-04-08"/><meta property="article:author" content="Johnny Dunn"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Logomaker: An experiment in human-computer interaction and ✨vibe coding ✨"/><meta name="twitter:description" content="Exploring the world of vibe coding through a logo creation tool, with insights on different LLMs and the changing landscape of software development."/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><div class="bg-[#1a1a1e]"><div class="container mx-auto px-4 py-10"><div class="flex flex-wrap items-center justify-between"><div class="flex items-center space-x-2"><a class="text-3xl font-bold bg-clip-text text-transparent bg-gradient-to-r from-teal-400 to-blue-500 glitch uppercase" href="/">manic <br/> Agency</a><span class="hidden md:inline-block h-6 w-px bg-gray-300 mx-2"></span><p class="hidden md:block text-xs text-gray-200 max-w-md">Intersection of reality, mixed reality, web3, and the emerging metaverse</p></div><nav class="hidden md:flex space-x-8 items-center"><a href="/#services" class="text-gray-100 hover:text-teal-300 transition duration-300">Services</a><a href="/projects" class="text-gray-100 hover:text-teal-300 transition duration-300">Projects</a><a href="/team" class="text-gray-100 hover:text-teal-300 transition duration-300">Maniacs</a><a href="/blog" class="text-gray-100 hover:text-teal-300 transition duration-300">Blog</a><a href="/open-source" class="text-gray-100 hover:text-teal-300 transition duration-300">Open-Source</a><a class="inline-block px-8 py-3 text-black font-mono text-lg tracking-wider uppercase bg-[#0ff] rounded-md transition duration-300 hover:bg-[#0cc] shadow-[4px_4px_0_#f0f,8px_8px_0_#00f] hover:shadow-[2px_2px_0_#f0f,4px_4px_0_#00f]" href="/contact">Contact Us</a></nav><div class="md:hidden"><button class="text-white focus:outline-none"><svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div></div></div><main class="blog-container"><article class="blog-post"><h1 class="blog-title">Logomaker: An experiment in human-computer interaction and ✨vibe coding ✨</h1><div class="blog-meta-container"><p class="blog-meta"><span>Johnny Dunn<!-- --> • </span><span class="blog-date-published" title="Published: 2025-04-08"><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg><time dateTime="2025-04-08">April 8, 2025</time></span><span class="reading-time"><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>24<!-- --> min read</span></p><a class="blog-category-link" href="/category/thinkpieces">Filed under: <span class="category-name">THINKPIECES</span></a></div><div class="blog-content"><div class="jsx-56bcf0b3bd83ed8a "><p node="[object Object]"><strong>GitHub link: <a href="https://github.com/manicinc/logomaker">https://github.com/manicinc/logomaker</a></strong></p>
<p node="[object Object]"><em><strong>Note: Each LLM tested (GPT-4o, GPT-4.5, GPT–o1, Claude Sonnet 3.7, Gemini 2.5 Pro) was done using the default settings (No extended thinking / deep research, no 200$ Pro subscription or web search experimental plugins or memories). I used the basic plan and default options for each (all set at 20$ a month currently). This is only called an experiment in title, as it is incredibly anecdotal. Everything was written / tested in VS Code with Copilot enabled and used to solve single line bugs.</strong></em></p>
<p node="[object Object]">At the time of writing this, I&#x27;ll have been in the software field for the upper half of but still quite far away decade amount of time. It might be a unpleasant shock to realize this declaration is necessary to introduce this post, because there are in fact non-junior engineers (as in, mid-level, as in, working for multiple years now) who <strong>just</strong> might have gotten away with not handwriting a class or file, function, or even line of code without the aid of generative AI.</p>
<p node="[object Object]">Last week, while working on one of our open-source projects PortaPack (<a href="https://github.com/manicinc/portapack">https://github.com/manicinc/portapack</a>), which is also being launched and released along with this post detailing our experiences building Logomaker (the two play nicely with our design philosophies of portability and software sustainability). I wanted to play around with some logo designs / typefaces before finalizing on a branding decision with the rest of our small team, who all work on their own projects, roles, and ventures, hence the usefulness of a rapid prototyping tool (<em><strong>self-reliance!</strong></em>).</p>
<div class="markdown-image-wrapper"><div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/portapack-logo.png" alt="The final version of the PortaPack logo, graphical" class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">The final version of the PortaPack logo, graphical</figcaption></figure></div></div><p node="[object Object]">
<em>The final version of the PortaPack logo, graphical.</em></p>
<p node="[object Object]">A cute, whimsical sort of feel was what I wanted. And it was a rough time finding something online I could get started with quickly. The strongly recommended recommendations for free logo makers coming in random threads almost always linked to gated paywalls and account subscriptions, oftentimes behind dark patterns, like being the next step before an image export after all the edits had been done by the user in a locked in UI, or limiting PNG quality exports to an clearly unusable amount.</p>
<p node="[object Object]">It&#x27;s common these days for tech projects to lock in their users into their software, and unfortunately also not be transparent about the limitations that they impose with those guards. Ones also specifically designed to elicit a payment, which oftentimes is just a one-time fee (as the first one is always the hardest one to get) making subscriptions and recurring payments for the actual loyal customers much more inconvenient. These are things that result in login screens and dashboard management features taking second precedence over new customers, or payment cancellation options behind hard to get to.</p>
<p node="[object Object]">Sheer frustration, a desire for a nice usable experience for something I wanted to do, and a stirring curiosity to see what would happen if we did things just to see what would happen brought me to pitch an idea to our devs: Vibe code an entire project, full-stack and fully usable--every function written by an LLM, every design done by an LLM. Besides, this is just what everybody in the world is going to start doing, if your sites and apps have a dreadful enough user experience.</p>
<p node="[object Object]">Logomaker sounds like a pretty good scope for this. It&#x27;s no fintech or healthcare app, the worst that happens is a user wastes their time trying a (unintendedly broken) site that has no ads and tracks no data. Hopefully it&#x27;s not unintendedly broken but who&#x27;s Q/Aing this stuff anyway? Logomaker, the app built 90% by ChatGPT? It&#x27;s Q/Aed by no one, use at your own peril.</p>
<div class="markdown-image-wrapper"><div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/logomaker-manic-example.png" alt="An example logo created with Logomaker" class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">An example logo created with Logomaker</figcaption></figure></div></div><p node="[object Object]">
<em>An example logo created with Logomaker</em></p>
<h2 id="llm-sees-llm-does"><a href="#llm-sees-llm-does">LLM sees, LLM does</a></h2>
<p node="[object Object]">I have a background going to an art and design college. But art (even just visual art) is so encompassing that logo designs are something I don&#x27;t think I ever studied. I have Photoshop and Illustrator experience, but rarely gave thought to how something like image editing software would actually work. So, none of the product features you see in the logo generator were pitched by me originally but they were refined.</p>
<p node="[object Object]">At the moment, this iterative product management process in giving product-driven prompts in addition to technical-guided ones was highly necessary, to create anything deemed worthy of being usable by a human being in 2025. On its own, the LLMs from Anthropic (Sonnet 3.7), ChatGPT (GPT-4o, GPT-o1, GPT-4.5), and Google Gemini (2.5 Pro), all of which were extensively tested and ✨vibe coded ✨ with throughout, could only go so far in self-improving their own code, styles, and features.</p>
<p node="[object Object]">Without human guidance at various points in this process, mapping out sensible AND robust user flows the way humans want to use software seems more difficult for LLMs than implementing very complex algorithms. Is this a limitation of something like a creativity mechanism in the LLM? Or is it a natural consequence of its training data? What happens if we get 10,000 product designers to write 10,000 user stories each? (100 billion user stories! This would entail in a model that really is about as large as or in the ballpark of GPT-3). Would the output of those models result in the most well-designed software the world&#x27;s ever known?</p>
<div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/alice-in-wonderland-using-tool-building.png" alt="Can we build it, LLMs?" class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">Can we build it, LLMs?</figcaption></figure></div>
<p node="[object Object]">For example, the LLMs of course knew what basic and desirable functionalities would go into a design tool like this, so of course exporting options were done (and fully working I might add, from the LLM writing the exact dependency links needed from the CDN link for html2canvas.js), and with multiple exporting options, though it was basic and naturally didn&#x27;t include SVG (which would be really complex, so it makes sense it&#x27;s originally ignored unless prompted).</p>
<p node="[object Object]">So, it&#x27;d be easy for me to simply ask for additional exporting options of GIF and SVG, which I did. But if I didn&#x27;t prompt the LLM to specifically design the addition of these new features in a way that, say, really considered the user experience, or even specifically instructed the LLM to do this, it would (typically) output the components to render a GIF, SVG, and PNG, but all 3 as just buttons with working functionality and no additional considerations in enhancing the UX. Tooltips, mobile responsive styles, etc. sure, it doesn&#x27;t go far beyond that though. It feels like, in general, LLMs like to be conservative in their token output / generation, which, in coding, isn&#x27;t good when you&#x27;re getting incomplete scripts, or, in many, many, many cases, placeholder logic that sneakily hides its way in there even when the LLM has been instructed aggressively to not output those comments.</p>
<p node="[object Object]">So, why even bother to offer different exporting options? What are the advantages of one or the other? SVGs are vector-based and scalable to any size and dimension. So SVGs are always better right?</p>
<p node="[object Object]">No, because it&#x27;s actually very hard to do things like programmatic animations of styles, etc. in a media type like SVG, because of its nature and complex implementation. So while CSS might be.. easy to style with (said with gritted teeth) and &quot;easy&quot; to export (well, it has its own quirks), good luck man at converting those accurately to SVG. Meaning, SVGs are nice for flexibility and GIFs are good for styling. This is a clear, straightforward distinction to make. And when you ask this to a LLM they, like most people familiar with this context, can give you that dead-on accurately.</p>
<p node="[object Object]">Here&#x27;s the issue. How can you guide a LLM to think about things like this, without specifically listing this type of thing as an example? Because, the thing about examples, is that when you have few or limited ones, you run into a limitation that is the same feature that empowers one-shot or few-shot learning (the ability for an LLM to learn relatively easily from a few examples just in the context of the prompt itself without actually retraining its data / model).</p>
<div class="markdown-image-wrapper"><div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/logomaker-old-version-first-one.png" alt="This is the first iteration of the &quot;ultimate logo generator&quot; which was all asked to be built and written in one file. The end result was just under 1000 lines." class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">This is the first iteration of the &quot;ultimate logo generator&quot; which was all asked to be built and written in one file. The end result was just under 1000 lines.</figcaption></figure></div></div><p node="[object Object]">
<em>This is the first iteration of the &quot;ultimate logo generator&quot; which was all asked to be built and written in one file. The end result was just under 1000 lines.</em></p>
<p node="[object Object]">This beginning code demonstrates the LLM &quot;generating&quot; the correct links for fonts (as well as other dependencies like <code class="px-1.5 py-0.5 bg-bg-tertiary text-accent-primary rounded font-mono" node="[object Object]">https://cdnjs.cloudflare.com/ajax/libs/gif.js/0.2.0/gif.worker.js</code>) in line 869, and starting the in-line CSS for styles for the logo creator to apply via UI selection.</p>
<pre><div class="rounded-md my-6" node="[object Object]" style="color:#e3eaf2;background:#111b27;font-family:Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:0.5em 0;overflow:auto"><code class="language-html" style="white-space:pre;color:#e3eaf2;background:none;font-family:Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token" style="color:#e3eaf2">&lt;!</span><span class="token doctype-tag" style="color:#8da1b9">DOCTYPE</span><span class="token" style="color:#8da1b9"> </span><span class="token name" style="color:#8da1b9">html</span><span class="token" style="color:#e3eaf2">&gt;</span><span>
</span><span></span><span class="token" style="color:#66cccc">&lt;</span><span class="token" style="color:#66cccc">html</span><span class="token" style="color:#66cccc"> </span><span class="token" style="color:#e6d37a">lang</span><span class="token attr-equals" style="color:#66cccc">=</span><span class="token" style="color:#66cccc">&quot;</span><span class="token" style="color:#91d076">en</span><span class="token" style="color:#66cccc">&quot;</span><span class="token" style="color:#66cccc">&gt;</span><span>
</span><span></span><span class="token" style="color:#66cccc">&lt;</span><span class="token" style="color:#66cccc">head</span><span class="token" style="color:#66cccc">&gt;</span><span>
</span><span>  </span><span class="token" style="color:#66cccc">&lt;</span><span class="token" style="color:#66cccc">meta</span><span class="token" style="color:#66cccc"> </span><span class="token" style="color:#e6d37a">charset</span><span class="token attr-equals" style="color:#66cccc">=</span><span class="token" style="color:#66cccc">&quot;</span><span class="token" style="color:#91d076">UTF-8</span><span class="token" style="color:#66cccc">&quot;</span><span class="token" style="color:#66cccc">&gt;</span><span>
</span><span>  </span><span class="token" style="color:#66cccc">&lt;</span><span class="token" style="color:#66cccc">title</span><span class="token" style="color:#66cccc">&gt;</span><span>Logo Generator</span><span class="token" style="color:#66cccc">&lt;/</span><span class="token" style="color:#66cccc">title</span><span class="token" style="color:#66cccc">&gt;</span><span>
</span><span>  </span><span class="token" style="color:#8da1b9">&lt;!-- Extended Google Fonts API --&gt;</span><span>
</span><span>  </span><span class="token" style="color:#66cccc">&lt;</span><span class="token" style="color:#66cccc">link</span><span class="token" style="color:#66cccc"> </span><span class="token" style="color:#e6d37a">rel</span><span class="token attr-equals" style="color:#66cccc">=</span><span class="token" style="color:#66cccc">&quot;</span><span class="token" style="color:#91d076">preconnect</span><span class="token" style="color:#66cccc">&quot;</span><span class="token" style="color:#66cccc"> </span><span class="token" style="color:#e6d37a">href</span><span class="token attr-equals" style="color:#66cccc">=</span><span class="token" style="color:#66cccc">&quot;</span><span class="token" style="color:#91d076">https://fonts.googleapis.com</span><span class="token" style="color:#66cccc">&quot;</span><span class="token" style="color:#66cccc">&gt;</span><span>
</span><span>  </span><span class="token" style="color:#66cccc">&lt;</span><span class="token" style="color:#66cccc">link</span><span class="token" style="color:#66cccc"> </span><span class="token" style="color:#e6d37a">rel</span><span class="token attr-equals" style="color:#66cccc">=</span><span class="token" style="color:#66cccc">&quot;</span><span class="token" style="color:#91d076">preconnect</span><span class="token" style="color:#66cccc">&quot;</span><span class="token" style="color:#66cccc"> </span><span class="token" style="color:#e6d37a">href</span><span class="token attr-equals" style="color:#66cccc">=</span><span class="token" style="color:#66cccc">&quot;</span><span class="token" style="color:#91d076">https://fonts.gstatic.com</span><span class="token" style="color:#66cccc">&quot;</span><span class="token" style="color:#66cccc"> </span><span class="token" style="color:#e6d37a">crossorigin</span><span class="token" style="color:#66cccc">&gt;</span><span>
</span><span>  </span><span class="token" style="color:#66cccc">&lt;</span><span class="token" style="color:#66cccc">link</span><span class="token" style="color:#66cccc"> </span><span class="token" style="color:#e6d37a">href</span><span class="token attr-equals" style="color:#66cccc">=</span><span class="token" style="color:#66cccc">&quot;</span><span class="token" style="color:#91d076">https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;700;900&amp;family=Audiowide&amp;family=Bungee+Shade&amp;family=Bungee&amp;family=Bungee+Outline&amp;family=Bungee+Hairline&amp;family=Chakra+Petch:wght@700&amp;family=Exo+2:wght@800&amp;family=Megrim&amp;family=Press+Start+2P&amp;family=Rubik+Mono+One&amp;family=Russo+One&amp;family=Syne+Mono&amp;family=VT323&amp;family=Wallpoet&amp;family=Faster+One&amp;family=Teko:wght@700&amp;family=Black+Ops+One&amp;family=Bai+Jamjuree:wght@700&amp;family=Righteous&amp;family=Bangers&amp;family=Raleway+Dots&amp;family=Monoton&amp;family=Syncopate:wght@700&amp;family=Lexend+Mega:wght@800&amp;family=Michroma&amp;family=Iceland&amp;family=ZCOOL+QingKe+HuangYou&amp;family=Zen+Tokyo+Zoo&amp;family=Major+Mono+Display&amp;family=Nova+Square&amp;family=Kelly+Slab&amp;family=Graduate&amp;family=Unica+One&amp;family=Aldrich&amp;family=Share+Tech+Mono&amp;family=Silkscreen&amp;family=Rajdhani:wght@700&amp;family=Jura:wght@700&amp;family=Goldman&amp;family=Tourney:wght@700&amp;family=Saira+Stencil+One&amp;family=Syncopate&amp;family=Fira+Code:wght@700&amp;family=DotGothic16&amp;display=swap</span><span class="token" style="color:#66cccc">&quot;</span><span class="token" style="color:#66cccc"> </span><span class="token" style="color:#e6d37a">rel</span><span class="token attr-equals" style="color:#66cccc">=</span><span class="token" style="color:#66cccc">&quot;</span><span class="token" style="color:#91d076">stylesheet</span><span class="token" style="color:#66cccc">&quot;</span><span class="token" style="color:#66cccc">&gt;</span><span>
</span><span>  </span><span class="token" style="color:#66cccc">&lt;</span><span class="token" style="color:#66cccc">style</span><span class="token" style="color:#66cccc">&gt;</span><span>
</span>    :root {
<!-- -->      --primary-gradient: linear-gradient(
<!-- -->        45deg, 
<!-- -->        #FF1493,   /* Deep Pink */
<!-- -->        #FF69B4,   /* Hot Pink */
<!-- -->        #FF00FF,   /* Magenta */
<!-- -->        #FF4500,   /* Orange Red */
<!-- -->        #8A2BE2    /* Blue Violet */
<!-- -->      );
<!-- -->      --cyberpunk-gradient: linear-gradient(
<!-- -->        45deg,
<!-- -->        #00FFFF, /* Cyan */
<!-- -->        #FF00FF, /* Magenta */
<!-- -->        #FFFF00  /* Yellow */
<!-- -->      );
<!-- -->      --sunset-gradient: linear-gradient(
<!-- -->        45deg,
<!-- -->        #FF7E5F, /* Coral */
<!-- -->        #FEB47B, /* Peach */
<!-- -->        #FF9966  /* Orange */
<!-- -->      );
<!-- -->      --ocean-gradient: linear-gradient(
<!-- -->        45deg,
<!-- -->        #2E3192, /* Deep Blue */
<!-- -->        #1BFFFF  /* Light Cyan */
<!-- -->      );
<!-- -->      --forest-gradient: linear-gradient(
<!-- -->        45deg,
<!-- -->        #134E5E, /* Deep Teal */
<!-- -->        #71B280  /* Light Green */
<!-- -->      );
<!-- -->      --rainbow-gradient: linear-gradient(
<!-- -->        45deg,
<!-- -->        #FF0000, /* Red */
<!-- -->        #FF7F00, /* Orange */
<!-- -->        #FFFF00, /* Yellow */
<!-- -->        #00FF00, /* Green */
<!-- -->        #0000FF, /* Blue */
<!-- -->        #4B0082, /* Indigo */
<!-- -->        #9400D3  /* Violet */
<!-- -->      );
<!-- -->    }
<!-- -->    ..</code></div></pre>
<p node="[object Object]"><strong>Full gist of the generated HTML / logic is at:
<a href="https://gist.github.com/jddunn/48bc03f3a9f85ffd8ccf90c801f6cf93">https://gist.github.com/jddunn/48bc03f3a9f85ffd8ccf90c801f6cf93</a></strong></p>
<p node="[object Object]">While I don&#x27;t have the original prompt that was used to create this file, the working version was generated in one-go (single round) with no prior context or examples of code given. In total, the prompt itself must&#x27;ve been a single paragraph long, and simply asked for a nicely designed and usable logo maker / generator that had export options and good styling options. Nothing was specified, and at the time, font management wasn&#x27;t decided on a feature yet.</p>
<p node="[object Object]">Unfortunately, the original plan was to utilize Aider, one of the more widely supported (updated) and widely used libraries for generative AI and coding. Aider advertises itself as the AI pair programmer assistant. It feels like you may use vibe coding to use Aider, but the act of using Aider itself isn&#x27;t necessarily vibe coding, nor is it an inherent act in any interaction with a LLM unless there&#x27;s an intentional collaborative framework done by the user. In other words, vibe coding is applicable when it has to be a user that&#x27;s testing the LLM&#x27;s suggested coding changes and verifying the output. It is not the user asking the LLM for code which the user then goes through and rewrites to fit into their system / codebase.</p>
<p node="[object Object]">But it&#x27;s also tricky, because you can go &quot;in and out&quot; of vibe coding like state phases. One feature or class or function or even LOC can be &quot;vibe coded&quot;, which simply should just imply the act of delegating more responsibility to the LLM to produce some working functionality than what the user assigns themselves. The dev becomes the pair programmer, instead of Aider, per se, being your pair programmer.</p>
<div class="markdown-image-wrapper"><div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/this-is-aider.png" alt="Aider interface" class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">Aider interface</figcaption></figure></div></div><p node="[object Object]">
<em>Aider interface</em></p>
<p node="[object Object]">That said, the reason why vibe coding wasn&#x27;t done through Aider simply had to do with the implementation of the newest upgrades of Aider itself. It was simply my / our team&#x27;s personal experience that the far older versions of Aider provided much more usable functionality. We did make a solid attempt as Aider can edit and write files directly on the file system (as can some extensions in VS Code, Cursor, and other frameworks, but here we&#x27;re just focusing on Aider as it seems like the current strongest contender), but after the first several edits we noticed functionality getting worse. But as we&#x27;ll get into soon, this type of thing is by no means an issue exclusive to Aider and programming aide tools like it. It&#x27;s an issue that naturally comes with the usability of all these LLMs, when we make the decision on interacting with them via an app, or via the website, or via the API, or via an agent API, etc., etc.</p>
<p node="[object Object]">So we took the original lines of code we were given by Aider (what you saw above in the first iteration), and sent it to Claude Sonnet 3.7, and what was supposed to be a 2 hour project became a 2 day project which became a 10 day project.</p>
<div class="markdown-image-wrapper"><div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/logomaker-claude-horror-chat-history.png" alt="Hello darkness my old friend" class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">Hello darkness my old friend</figcaption></figure></div></div><p node="[object Object]">
<em>Hello darkness my old friend</em></p>
<p node="[object Object]">This is only the conversations list on Anthropic&#x27;s Claude&#x27;s UI (since this is the nicest looking and one with the most organized search). We used OpenAI&#x27;s ChatGPT and Google Gemini&#x27;s Pro paid plans, not just to test and compare, but because we had to. This thing still isn&#x27;t done fully bug-free after 10 days!</p>
<p node="[object Object]">Don&#x27;t want to add any new classes or fix any functions fully by hand, when we know darn well what needs to be fixed and what the LLM is continually doing and redoing wrong over and over again? That&#x27;s not quite the vibe we&#x27;re hoping to catch from the vibe coding experiment.</p>
<h2 id="how-to-vibe-with-vibe-coding-vibes"><a href="#how-to-vibe-with-vibe-coding-vibes">How to vibe with vibe coding vibes?</a></h2>
<div class="markdown-image-wrapper"><div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/logomaker-claude-demonstrates-coding-ability-1.png" alt="This type of prompt is not completely recommended but probably works well enough. Actually the curtness was intentional to see if Claude could extrapolate better functionality from just short instructions, which is how most casual users would try this, compared to something in-depth." class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">This type of prompt is not completely recommended but probably works well enough. Actually the curtness was intentional to see if Claude could extrapolate better functionality from just short instructions, which is how most casual users would try this, compared to something in-depth.</figcaption></figure></div></div><p node="[object Object]">
<em>This type of prompt is not completely recommended but probably works well enough. Actually the curtness was intentional to see if Claude could extrapolate better functionality from just short instructions, which is how most casual users would try this, compared to something in-depth.</em></p>
<p node="[object Object]">Claude generally always generates files in the right format, whether it&#x27;s JavaScript, Python, Markdown, etc. Gemini does a great job with this too, though Anthropic&#x27;s UI / UX far outclasses Gemini.</p>
<div class="markdown-image-wrapper"><div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/logomaker-claude-demonstrates-coding-ability-2.png" alt="Claude&#x27;s response showing code generation capabilities" class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">Claude&#x27;s response showing code generation capabilities</figcaption></figure></div></div><p node="[object Object]">
<em>Claude&#x27;s response showing code generation capabilities</em></p>
<p node="[object Object]">You see we hit limits with Claude, of course, as we still desperately cling to the hope that we can just keep this constrained in one file, and be usable enough to be fun and decent. Plus, let&#x27;s just see how far we can push these generations. Claude says we can just say &quot;continue&quot; and it&#x27;ll work. Will it? (Hint: It didn&#x27;t for OpenAI&#x27;s GPT-4o models oftentimes, but Anthropic&#x27;s UI is king as we&#x27;ve said).</p>
<div class="markdown-image-wrapper"><div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/logomaker-claude-demonstrates-coding-ability-3.png" alt="Getting closer, but we&#x27;re still not quite there yet.." class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">Getting closer, but we&#x27;re still not quite there yet..</figcaption></figure></div></div><p node="[object Object]">
<em>Getting closer, but we&#x27;re still not quite there yet..</em></p>
<p node="[object Object]">Alright, let&#x27;s just.. continue..</p>
<div class="markdown-image-wrapper"><div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/logomaker-claude-demonstrates-coding-ability-4.png" alt="Getting closer, but we&#x27;re still not quite there yet.." class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">Getting closer, but we&#x27;re still not quite there yet..</figcaption></figure></div></div><p node="[object Object]">
<em>Getting closer, but we&#x27;re still not quite there yet..</em></p>
<p node="[object Object]">Okay, we started out with an 850 line file that actually gave us a fully functional app. Working PNG renders and working logos. This did prove my original theory and that I&#x27;m not completely delusional. I said to myself I&#x27;ve been wasting so much time in dead end dark patterns trying to find a free logo generator just to do some fun experimenting with, that it might be more efficient just to vibe code one and like <del>magic</del> it appears.. And to prove it, I got a fully working app (HTML with inline CSS / JS counts!) in 1-3 prompts from Aider using GPT-4o, that&#x27;s incredibly limited and minimal sure, but truthfully did offer more functionality than the &quot;free&quot; demos these other sites were offering. And while that was written with Aider at first, the underlying LLM models are the same, and without a doubt (at the moment), in my experiences, OpenAI does a much superior job in responding to the user through the UI than giving the same prompt to the same model in Aider.</p>
<p node="[object Object]">And after asking Claude to simply improve it, we were left with almost double the LOC! But it doesn&#x27;t compile because it&#x27;s not finished, so we can&#x27;t use it. And despite what Claude says in the UI, we are simply unable to continue any further, with this line of prompting (&quot;continue&quot;), to progress.</p>
<p node="[object Object]">We know Claude and OpenAI can go into context windows of 100-200k, but apparently, that seems to only be in the Extended Mode. So what does this &quot;continue&quot; button even do? And what is this &quot;Extended Mode&quot;, is this what we&#x27;re forced into since the &quot;continue&quot; button doesn&#x27;t work? Is it summarizing my conversation? Is it using Claude again to summarize my conversation? Is it aggregating the last 10 or so messages or however many until it reaches a predetermined limit (and how does it determine this limit, is it limiting my output window size thus limiting the ability for me to use Claude for pair programming?)?</p>
<p node="[object Object]">Outputs for LLMs are typically capped at 8,192 tokens, which is highly standard (as well as a completely arbitrarily defined number, one that can easily be extended by these respective API / LLM providers, and oftentimes is). The context windows are the same.</p>
<h2 id="the-real-world-the-real-problems"><a href="#the-real-world-the-real-problems">The real world, the real problems</a></h2>
<p node="[object Object]">The more relevant issue here is just how confusing and opaque these tools and the additional options being offered to us work. These are features that are actively costing us money. And time. So a lot of money actually (for a lot of working engineers). When I use ChatGPT-4.5, the more expensive model and take the time to move my conversational context and project info there, and accept the eventual price hikes that will come with these &quot;upgrades&quot; (surely, when they do..), I want to know what this is doing better, and maybe more importantly, why? Why is this model better, so I can actually make an informed decision on what to use? If I use X many calls from this model, am I limited then in my calls to other models? Oh no, I have to ask ChatGPT how much it costs to use ChatGPT?</p>
<div class="markdown-image-wrapper"><div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/chatgpt-o1-you-hit-rate-limit.png" alt="I wasn&#x27;t told I had hit my rate limit, or was coming anywhere near it during this conversation. Claude again has a better UX experience here as they warn you when you are beginning a long conversation that will quickly eat up your available credits" class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">I wasn&#x27;t told I had hit my rate limit, or was coming anywhere near it during this conversation. Claude again has a better UX experience here as they warn you when you are beginning a long conversation that will quickly eat up your available credits</figcaption></figure></div></div><p node="[object Object]">
<em>I wasn&#x27;t told I had hit my rate limit, or was coming anywhere near it during this conversation. Claude again has a better UX experience here as they warn you when you are beginning a long conversation that will quickly eat up your available credits</em></p>
<p node="[object Object]">Here&#x27;s the distinction between a bug / error that&#x27;s okay and one that&#x27;s totally not. This is not a rate limit in the output of tokens generated, as in, it hit a limit in writing the script I asked for, had to stop, and would or could potentially continue finishing writing that script once my usage had renewed.</p>
<p node="[object Object]">No, OpenAI&#x27;s UI simply did not respond when I inputted my prompt to patiently await the LLM&#x27;s response. Now this normally isn&#x27;t an issue, and I swear I can remember ChatGPT&#x27;s UI even used to say to just re-enter (nothin) in the chat window if no output was shown. But, when you are charging &quot;premium&quot; access for models, and heavily rate-limiting traffic to the point where every message has value, every few hours of waiting and refreshing of credits (in the case of Claude) is something to watch out for so you can continue utilizing these innovative tools, you can&#x27;t simply not show a response, and not show why. You as the organization and provider should eat the cost and re-generate, even despite the fact that it damages the conversational flow, memory, and context window, because at least then you allow the user to continue on with developing without introducing roadblocks that are inherent to the tools that you are asking themselves to essentially marry themselves to as they get far enough along in development.</p>
<p node="[object Object]">The nice thing about Google&#x27;s UI with Gemini? Despite it being an absolute menace, resource hog that somehow is 10x slower on Chrome than Firefox for me, and an all-around eyesore, is when there&#x27;s no response for an output, you can at least select an arrow button that shows the reasoning the LLM took to create that.. null or empty response. And that reasoning at least gives you a better understanding of what the LLM was &quot;thinking&quot; and oftentimes exactly what it was going to send to the user as its final output. It just like, chose not to?</p>
<div class="markdown-image-wrapper"><div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/google-gemini-pro-show-thinking-1.png" alt="By selecting the Show thinking button, you can see the exact reasoning the LLM is taking (note: it could be a series of calls as we have no transparency to what is happening in the web UI of Gemini) to answer. Which, oftentimes shows you the expected answer it was going to give but somehow didn&#x27;t. Seeing as these issues are parallel across different LLM web UIs (OpenAI, Claude, and Gemini) through testing, the issue most likely seems inherent to LLM architecture and response mechanisms." class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">By selecting the Show thinking button, you can see the exact reasoning the LLM is taking (note: it could be a series of calls as we have no transparency to what is happening in the web UI of Gemini) to answer. Which, oftentimes shows you the expected answer it was going to give but somehow didn&#x27;t. Seeing as these issues are parallel across different LLM web UIs (OpenAI, Claude, and Gemini) through testing, the issue most likely seems inherent to LLM architecture and response mechanisms.</figcaption></figure></div></div><p node="[object Object]">
<em>By selecting the Show thinking button, you can see the exact reasoning the LLM is taking (note: it could be a series of calls as we have no transparency to what is happening in the web UI of Gemini) to answer. Which, oftentimes shows you the expected answer it was going to give but somehow didn&#x27;t. Seeing as these issues are parallel across different LLM web UIs (OpenAI, Claude, and Gemini) through testing, the issue most likely seems inherent to LLM architecture and response mechanisms.</em></p>
<div class="markdown-image-wrapper"><div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/google-gemini-pro-show-thinking-2.png" alt="Google Gemini Pro&#x27;s thinking feature in action" class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">Google Gemini Pro&#x27;s thinking feature in action</figcaption></figure></div></div><p node="[object Object]">
<em>Google Gemini Pro&#x27;s thinking feature in action</em></p>
<p node="[object Object]">And for comparison&#x27;s sake, ChatGPT&#x27;s UI is by far the least consistent in terms of delivering consistent file formatting. ChatGPT finds it actually IMPOSSIBLE to deliver a single markdown file without messing up its formatting. Kidding, as it&#x27;s likely just the devs behind this wilding entity messing up the building the UI empowering it to exist.</p>
<div class="markdown-image-wrapper"><div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/chatgpt-not-markdown.png" alt="ChatGPT&#x27;s inconsistent markdown formatting" class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">ChatGPT&#x27;s inconsistent markdown formatting</figcaption></figure></div></div><p node="[object Object]">
<em>ChatGPT&#x27;s inconsistent markdown formatting</em></p>
<p node="[object Object]">That&#x27;s not the file fully in markdown actually. Markdown should just literally look like a text file with special formatting characters.</p>
<div class="markdown-image-wrapper"><div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/chatgpt-not-markdown-2.png" alt="More markdown formatting issues with ChatGPT" class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">More markdown formatting issues with ChatGPT</figcaption></figure></div></div><p node="[object Object]">
<em>More markdown formatting issues with ChatGPT</em></p>
<p node="[object Object]">That&#x27;s also, just partially markdown, not all markdown.</p>
<div class="markdown-image-wrapper"><div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/chatgpt-not-giving-full-file.png" alt="ChatGPT giving a &quot;full&quot; refactored file from a script that was originally 1500 lines of code in JS" class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">ChatGPT giving a &quot;full&quot; refactored file from a script that was originally 1500 lines of code in JS</figcaption></figure></div></div><p node="[object Object]">
<em>ChatGPT giving a &quot;full&quot; refactored file from a script that was originally 1500 lines of code in JS</em></p>
<p node="[object Object]">That&#x27;s ChatGPT giving me a &quot;full&quot; refactored file from a script that was originally 1500 lines of code in JS (don&#x27;t judge, this is an LLM-generated or &quot;vibe-coded&quot; project remember?). It refactored it into 200 lines. It refactored like losing weight by cutting a limb off. Claude runs into the same issues we&#x27;ve seen earlier with its &quot;continue&quot; limit, which genuinely seems to be a UI limitation, which is very unfortunate, as Sonnet 3.7 (at the moment) was doing great work up until it hit its limits. Gemini Pro 2.5? This was the only model capable of generating a full ~2000 LOC file coherently with minimal hallucinations in one go.</p>
<p node="[object Object]">Here is a hint with ChatGPT, our OG LLM provider. If it asks you &quot;Would you like this answered in chat?&quot;, instead of it writing in a file in a text editor inside the chat window (which is what would be happening here), you click on that thing as soon as you&#x27;re able to before this other terrifying UI feature starts controlling your conversation and showing files rendered in the most unclear way.</p>
<p node="[object Object]">Though, I must emphasize, at the moment, as with anything with these APIs and providers, it seems everything is always subject to change, sometimes even at the whim of competitors:</p>
<ul>
<li><a href="https://www.reddit.com/r/Bard/comments/1idmqul/google_really_wants_to_punish_openai_for_that_one/">Google really wants to punish OpenAI for that one</a></li>
<li><a href="https://www.reddit.com/r/technology/comments/1co9lcg/openai_plans_to_announce_google_search_competitor/">OpenAI plans to announce Google search competitor</a></li>
<li><a href="https://www.reddit.com/r/OpenAI/comments/1e8mfmx/google_faked_the_release_date_for_the_updates_and/">Google faked the release date for the updates</a></li>
</ul>
<p node="[object Object]">Somehow this transparency of showing thinking / reasoning from Gemini Pro also demonstrates the fundamental lack of transparency these platforms by design invite. Why show me the thought process if I don&#x27;t understand how that thinking works? Is it just like, 4 API calls on top of each other? Does that mean it uses 4x as many &quot;credits&quot; as I would have in my plan then?</p>
<p node="[object Object]">Devs behind these providers may just try new features or A/B experiments, and you might not have any idea about a change until it starts to go trending on Reddit, Twitter, etc.</p>
<ul>
<li><a href="https://www.reddit.com/r/OpenAI/comments/1jlwhs0/was_gpt4o_nerfed_again/">Was GPT-4o nerfed again?</a></li>
<li><a href="https://www.reddit.com/r/singularity/comments/1gy7p1d/boys_what_openai_did_to_this_model/">Boys what OpenAI did to this model</a></li>
<li><a href="https://www.reddit.com/r/ChatGPT/comments/1iu237v/openai_nerfing_gpt_feels_like_a_major_downgrade/">OpenAI nerfing GPT feels like a major downgrade</a></li>
<li><a href="https://news.ycombinator.com/item?id=40077683">Hacker News discussion on nerfing</a></li>
<li><a href="https://forum.cursor.com/t/claude-3-7-max-been-nerfed/73840">Claude 3.7 Max been nerfed?</a></li>
<li><a href="https://www.threads.net/@sobri909/post/DH-P4irxjrU/yeah-whenever-people-say-x-model-has-been-nerfed-its-almost-aways-complete-bulls">Whenever people say X model has been nerfed it&#x27;s almost always complete bulls**t</a></li>
<li><a href="https://news.ycombinator.com/item?id=41327360">Hacker News item 41327360</a></li>
<li><a href="https://x.com/samim/status/1876005616403300582">Twitter discussion on model changes</a></li>
</ul>
<p node="[object Object]">It&#x27;s clear the user community around AI has a lot of fears that don&#x27;t involve becoming obsolete by singularity or automation. Users are heavily embracing generative AI, at an almost alarming rate.</p>
<div class="markdown-image-wrapper"><div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/her-movie-screenshot-warner-bros.png" alt="Scene from the movie &quot;Her&quot; by Warner Bros" class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">Scene from the movie &quot;Her&quot; by Warner Bros</figcaption></figure></div></div><p node="[object Object]">
<em>Scene from the movie &quot;Her&quot; by Warner Bros</em></p>
<p node="[object Object]">And we&#x27;re still left in the dark in other ways. How much of a competitive edge do companies like OpenAI and Anthropic get when they can internally adjust the outputs and filters / censors of their models at will? How much access is available for large organizations (governments, banks, hedge funds, or huge tech companies with their own silos like Oracle, Microsoft) to &quot;buy&quot; control, even temporary or one-time arrangements, over these inputs and outputs that are completely black-box to your average user?</p>
<div class="markdown-image-wrapper"><div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/logomaker-live-site.png" alt="The live site of Logomaker, at https://manicinc.github.io/logomaker, which will live here free forever so long as GitHub Pages is free." class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">The live site of Logomaker, at https://manicinc.github.io/logomaker, which will live here free forever so long as GitHub Pages is free.</figcaption></figure></div></div><p node="[object Object]">
<em>The live site of Logomaker, at <a href="https://manicinc.github.io/logomaker">https://manicinc.github.io/logomaker</a>, which will live here free forever so long as GitHub Pages is free.</em></p>
<div class="md-image-container"><figure class="image-with-caption mx-auto block"><img src="/assets/blog/logomaker-live-site-2.png" alt="Logomaker live site 2" class="content-image w-full my-8 mx-auto block  rounded-md" loading="lazy" node="[object Object]"/><figcaption class="text-center text-sm text-text-muted mt-2">Logomaker live site 2</figcaption></figure></div>
<p node="[object Object]">This app was built as an experimental work to test the current capabilities of different LLMs as well as their providers and the accompanying UI features serviced by them. It&#x27;s meant as a fun, useful, and chaotic work where the dev was fully dedicated to just using vibe coding, or allowing the LLM to generate code and functions with detailed and clear technical guidance. Individual small fixes for issues from the LLM providers (the code given out in the UIs) were fixed sometimes with GitHub Co-Pilot for convenience&#x27;s (and budget) sake.</p>
<p node="[object Object]">Originally, the hope was to get this whole thing done in just 1 HTML file! And not take so many multiple days (working on and off) to finish up. And it was, in just a few prompts too. We&#x27;ll post that as a snippet here: <a href="coming-soon">coming-soon</a>. But, it just seemed like every new feature was just a quick prompt or two away, and so on, and so on, and..</p>
<p node="[object Object]">So one day, we had an intelligent <a href="#">font management system</a> that could lazily load gigabytes of fonts in a speedy way, a <a href="#">build setup</a> that worked with our other <a href="#">portapack-package</a> and could <a href="#">compile</a> into an Electron app, a live dev workflow.</p>
<p node="[object Object]">Then the next day, we had full SVG support. And not just support for static SVGs, but actual animations! Something really difficult to pull off in imaging applications, and something I had never even thought about (or had any idea on how to implement, as all those algorithmic and style building / XML techniques were done by the LLM, with no external sources / documentation given for reference).</p>
<p node="[object Object]">And so on and on with each passing day, until a 2 hour project became a 2 day project which became a 10 day project which is the full-fledged design playground application you see linked below.</p>
<p node="[object Object]">We kept things simple, static assets, all client-side rendering, no server required to run the app (not necessarily, anyway, as we have a multitude of building / running options), and, just JavaScript. No TypeScript, unit tests, or refactoring that wasn&#x27;t done on-the-fly. This wasn&#x27;t a scientific experiment, but given every function was definitively written by an LLM (by intention), it&#x27;s safe to say well over 90% of the codebase was done by generative AI. At least 80% of the docs you see in the repo were written by generative AI. And 0% of this article was written by generative AI.</p>
<p node="[object Object]">What do you think about the source code, designs, and end results that these large language models, these AI assistants, these slaves workers collaborators agents did?</p>
<ul>
<li><a href="https://manicinc.github.io/logomaker">Live Demo: https://manicinc.github.io/logomaker</a></li>
<li><a href="https://github.com/manicinc/logomaker">GitHub Repo: https://github.com/manicinc/logomaker</a></li>
</ul></div></div><div class="blog-tags"><a class="blog-tag" href="/tags/featured">#<!-- -->featured</a></div></article><div class="blog-navigation"><a class="back-to-blog" href="/blog">← Back to all posts</a></div></main><footer class="bg-gradient-to-r from-[#23153c] to-[#1e1b45] text-white py-12 px-6 md:px-16"><div class="max-w-7xl mx-auto grid grid-cols-1 md:grid-cols-4 gap-8"><div><h3 class="text-lg font-semibold mb-4">Our Team Locations</h3><ul class="space-y-2 opacity-80"><li>U.S.A</li><li>Los Angeles, California</li><li>Lagos, Nigeria</li><li class="mt-2 text-purple-300">team@manic.agency</li></ul></div><div><h3 class="text-lg font-semibold mb-4">Products</h3><ul class="space-y-2 opacity-80"><li>Velvet Web</li><li>Smart Parser</li><li>SynthGPT</li><li class="mt-2 text-purple-300 cursor-pointer hover:underline">See all →</li></ul></div><div><h3 class="text-lg font-semibold mb-4">Company</h3><ul class="space-y-2 opacity-80"><li><a class="hover:text-purple-300" href="/mission">Mission</a></li><li><a class="hover:text-purple-300" href="/work">Work</a></li><li><a class="hover:text-purple-300" href="/process">Process</a></li><li><a class="hover:text-purple-300" href="/blog">Blog</a></li><li><a class="hover:text-purple-300" href="/contact">Contact us</a></li><li><a class="hover:text-purple-300" href="/team">Team</a></li></ul></div><div><h3 class="text-lg font-semibold mb-4">Connect</h3><div class="flex space-x-4 text-xl"><a href="https://github.com/manicinc" class="hover:text-purple-300" target="_blank"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://www.linkedin.com/company/manic-agency-llc/" class="hover:text-purple-300" target="_blank"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a href="https://x.com/manicagency" class="hover:text-purple-300" target="_blank"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a></div></div></div><div class="border-t border-white/20 mt-8 pt-6 text-center text-sm opacity-80"><p class="font-semibold text-lg">Manic Agency</p><p class="text-purple-300">team@manic.agency</p><p>Los Angeles California</p><p class="mt-4">© Manic Agency 2025</p></div></footer><script src="/_next/static/chunks/webpack-97daa47057c0ad96.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[94970,[],\"ClientSegmentRoot\"]\n3:I[9322,[\"344\",\"static/chunks/9081a741-c64b490c58328843.js\",\"874\",\"static/chunks/874-dce3bf739b01de3a.js\",\"830\",\"static/chunks/830-9c5a0465a430e144.js\",\"177\",\"static/chunks/app/layout-364d573b212e082c.js\"],\"default\"]\n4:I[87555,[],\"\"]\n5:I[31295,[],\"\"]\n6:I[54824,[\"874\",\"static/chunks/874-dce3bf739b01de3a.js\",\"63\",\"static/chunks/63-d95d5592655c601c.js\",\"741\",\"static/chunks/741-a8214fa2992e7fbd.js\",\"345\",\"static/chunks/app/not-found-7ced8bb09ca0d298.js\"],\"default\"]\n9:I[59665,[],\"OutletBoundary\"]\nc:I[59665,[],\"ViewportBoundary\"]\ne:I[59665,[],\"MetadataBoundary\"]\n10:I[26614,[],\"\"]\n:HL[\"/_next/static/css/5e3bf2e3cf44f85c.css\",\"style\"]\n:HL[\"/_next/static/css/6f1e80598b39c648.css\",\"style\"]\n:HL[\"/_next/static/css/e06744435527e09b.css\",\"style\"]\n:HL[\"/_next/static/css/f1b33108a809bcd7.css\",\"style\"]\n:HL[\"/_next/static/css/9922de4f935ecb48.css\",\"style\"]\n:HL[\"/_next/static/css/573f7debcb2da6f3.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"yvYcpXOFfe5jdHjmchdJf\",\"p\":\"\",\"c\":[\"\",\"blog\",\"thinkpieces\",\"logomaker-an-experiment-in-human-computer-interaction-vibe-coding\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"category\",\"thinkpieces\",\"d\"],{\"children\":[[\"slug\",\"logomaker-an-experiment-in-human-computer-interaction-vibe-coding\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/5e3bf2e3cf44f85c.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/6f1e80598b39c648.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/e06744435527e09b.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"3\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/f1b33108a809bcd7.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"4\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/9922de4f935ecb48.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"5\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/573f7debcb2da6f3.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L2\",null,{\"Component\":\"$3\",\"slots\":{\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"$L6\",null,{}],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]},\"params\":{},\"promise\":\"$@7\"}]]}],{\"children\":[\"blog\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"category\",\"thinkpieces\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"logomaker-an-experiment-in-human-computer-interaction-vibe-coding\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L8\",\"$undefined\",null,[\"$\",\"$L9\",null,{\"children\":[\"$La\",\"$Lb\",null]}]]}],{},null,false]},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"-pVPodcOtfJlG3yFJEwEO\",{\"children\":[[\"$\",\"$Lc\",null,{\"children\":\"$Ld\"}],null]}],[\"$\",\"$Le\",null,{\"children\":\"$Lf\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$10\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"7:{}\n"])</script><script>self.__next_f.push([1,"11:I[48231,[\"874\",\"static/chunks/874-dce3bf739b01de3a.js\",\"932\",\"static/chunks/932-0b16ec775ea3eed2.js\",\"616\",\"static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-1a65ae41b9c69b5b.js\"],\"Nav\"]\n12:I[6874,[\"874\",\"static/chunks/874-dce3bf739b01de3a.js\",\"932\",\"static/chunks/932-0b16ec775ea3eed2.js\",\"616\",\"static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-1a65ae41b9c69b5b.js\"],\"\"]\n13:I[90882,[\"874\",\"static/chunks/874-dce3bf739b01de3a.js\",\"932\",\"static/chunks/932-0b16ec775ea3eed2.js\",\"616\",\"static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-1a65ae41b9c69b5b.js\"],\"CustomMarkdownRenderer\"]\n14:T7b8f,"])</script><script>self.__next_f.push([1,"\n**GitHub link: [https://github.com/manicinc/logomaker](https://github.com/manicinc/logomaker)**\n\n***Note: Each LLM tested (GPT-4o, GPT-4.5, GPT–o1, Claude Sonnet 3.7, Gemini 2.5 Pro) was done using the default settings (No extended thinking / deep research, no 200$ Pro subscription or web search experimental plugins or memories). I used the basic plan and default options for each (all set at 20$ a month currently). This is only called an experiment in title, as it is incredibly anecdotal. Everything was written / tested in VS Code with Copilot enabled and used to solve single line bugs.***\n\nAt the time of writing this, I'll have been in the software field for the upper half of but still quite far away decade amount of time. It might be a unpleasant shock to realize this declaration is necessary to introduce this post, because there are in fact non-junior engineers (as in, mid-level, as in, working for multiple years now) who **just** might have gotten away with not handwriting a class or file, function, or even line of code without the aid of generative AI.\n\nLast week, while working on one of our open-source projects PortaPack ([https://github.com/manicinc/portapack](https://github.com/manicinc/portapack)), which is also being launched and released along with this post detailing our experiences building Logomaker (the two play nicely with our design philosophies of portability and software sustainability). I wanted to play around with some logo designs / typefaces before finalizing on a branding decision with the rest of our small team, who all work on their own projects, roles, and ventures, hence the usefulness of a rapid prototyping tool (***self-reliance!***).\n\n![The final version of the PortaPack logo, graphical](/assets/blog/portapack-logo.png)\n*The final version of the PortaPack logo, graphical.*\n\nA cute, whimsical sort of feel was what I wanted. And it was a rough time finding something online I could get started with quickly. The strongly recommended recommendations for free logo makers coming in random threads almost always linked to gated paywalls and account subscriptions, oftentimes behind dark patterns, like being the next step before an image export after all the edits had been done by the user in a locked in UI, or limiting PNG quality exports to an clearly unusable amount.\n\nIt's common these days for tech projects to lock in their users into their software, and unfortunately also not be transparent about the limitations that they impose with those guards. Ones also specifically designed to elicit a payment, which oftentimes is just a one-time fee (as the first one is always the hardest one to get) making subscriptions and recurring payments for the actual loyal customers much more inconvenient. These are things that result in login screens and dashboard management features taking second precedence over new customers, or payment cancellation options behind hard to get to.\n\nSheer frustration, a desire for a nice usable experience for something I wanted to do, and a stirring curiosity to see what would happen if we did things just to see what would happen brought me to pitch an idea to our devs: Vibe code an entire project, full-stack and fully usable--every function written by an LLM, every design done by an LLM. Besides, this is just what everybody in the world is going to start doing, if your sites and apps have a dreadful enough user experience.\n\nLogomaker sounds like a pretty good scope for this. It's no fintech or healthcare app, the worst that happens is a user wastes their time trying a (unintendedly broken) site that has no ads and tracks no data. Hopefully it's not unintendedly broken but who's Q/Aing this stuff anyway? Logomaker, the app built 90% by ChatGPT? It's Q/Aed by no one, use at your own peril.\n\n![An example logo created with Logomaker](/assets/blog/logomaker-manic-example.png)\n*An example logo created with Logomaker*\n\n## LLM sees, LLM does\n\nI have a background going to an art and design college. But art (even just visual art) is so encompassing that logo designs are something I don't think I ever studied. I have Photoshop and Illustrator experience, but rarely gave thought to how something like image editing software would actually work. So, none of the product features you see in the logo generator were pitched by me originally but they were refined.\n\nAt the moment, this iterative product management process in giving product-driven prompts in addition to technical-guided ones was highly necessary, to create anything deemed worthy of being usable by a human being in 2025. On its own, the LLMs from Anthropic (Sonnet 3.7), ChatGPT (GPT-4o, GPT-o1, GPT-4.5), and Google Gemini (2.5 Pro), all of which were extensively tested and ✨vibe coded ✨ with throughout, could only go so far in self-improving their own code, styles, and features.\n\nWithout human guidance at various points in this process, mapping out sensible AND robust user flows the way humans want to use software seems more difficult for LLMs than implementing very complex algorithms. Is this a limitation of something like a creativity mechanism in the LLM? Or is it a natural consequence of its training data? What happens if we get 10,000 product designers to write 10,000 user stories each? (100 billion user stories! This would entail in a model that really is about as large as or in the ballpark of GPT-3). Would the output of those models result in the most well-designed software the world's ever known?\n\n![Can we build it, LLMs?](/assets/blog/alice-in-wonderland-using-tool-building.png)\n\n\nFor example, the LLMs of course knew what basic and desirable functionalities would go into a design tool like this, so of course exporting options were done (and fully working I might add, from the LLM writing the exact dependency links needed from the CDN link for html2canvas.js), and with multiple exporting options, though it was basic and naturally didn't include SVG (which would be really complex, so it makes sense it's originally ignored unless prompted).\n\nSo, it'd be easy for me to simply ask for additional exporting options of GIF and SVG, which I did. But if I didn't prompt the LLM to specifically design the addition of these new features in a way that, say, really considered the user experience, or even specifically instructed the LLM to do this, it would (typically) output the components to render a GIF, SVG, and PNG, but all 3 as just buttons with working functionality and no additional considerations in enhancing the UX. Tooltips, mobile responsive styles, etc. sure, it doesn't go far beyond that though. It feels like, in general, LLMs like to be conservative in their token output / generation, which, in coding, isn't good when you're getting incomplete scripts, or, in many, many, many cases, placeholder logic that sneakily hides its way in there even when the LLM has been instructed aggressively to not output those comments.\n\nSo, why even bother to offer different exporting options? What are the advantages of one or the other? SVGs are vector-based and scalable to any size and dimension. So SVGs are always better right?\n\nNo, because it's actually very hard to do things like programmatic animations of styles, etc. in a media type like SVG, because of its nature and complex implementation. So while CSS might be.. easy to style with (said with gritted teeth) and \"easy\" to export (well, it has its own quirks), good luck man at converting those accurately to SVG. Meaning, SVGs are nice for flexibility and GIFs are good for styling. This is a clear, straightforward distinction to make. And when you ask this to a LLM they, like most people familiar with this context, can give you that dead-on accurately.\n\nHere's the issue. How can you guide a LLM to think about things like this, without specifically listing this type of thing as an example? Because, the thing about examples, is that when you have few or limited ones, you run into a limitation that is the same feature that empowers one-shot or few-shot learning (the ability for an LLM to learn relatively easily from a few examples just in the context of the prompt itself without actually retraining its data / model).\n\n![This is the first iteration of the \"ultimate logo generator\" which was all asked to be built and written in one file. The end result was just under 1000 lines.](/assets/blog/logomaker-old-version-first-one.png)\n*This is the first iteration of the \"ultimate logo generator\" which was all asked to be built and written in one file. The end result was just under 1000 lines.*\n\nThis beginning code demonstrates the LLM \"generating\" the correct links for fonts (as well as other dependencies like `https://cdnjs.cloudflare.com/ajax/libs/gif.js/0.2.0/gif.worker.js`) in line 869, and starting the in-line CSS for styles for the logo creator to apply via UI selection.\n \n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n  \u003cmeta charset=\"UTF-8\"\u003e\n  \u003ctitle\u003eLogo Generator\u003c/title\u003e\n  \u003c!-- Extended Google Fonts API --\u003e\n  \u003clink rel=\"preconnect\" href=\"https://fonts.googleapis.com\"\u003e\n  \u003clink rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin\u003e\n  \u003clink href=\"https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;700;900\u0026family=Audiowide\u0026family=Bungee+Shade\u0026family=Bungee\u0026family=Bungee+Outline\u0026family=Bungee+Hairline\u0026family=Chakra+Petch:wght@700\u0026family=Exo+2:wght@800\u0026family=Megrim\u0026family=Press+Start+2P\u0026family=Rubik+Mono+One\u0026family=Russo+One\u0026family=Syne+Mono\u0026family=VT323\u0026family=Wallpoet\u0026family=Faster+One\u0026family=Teko:wght@700\u0026family=Black+Ops+One\u0026family=Bai+Jamjuree:wght@700\u0026family=Righteous\u0026family=Bangers\u0026family=Raleway+Dots\u0026family=Monoton\u0026family=Syncopate:wght@700\u0026family=Lexend+Mega:wght@800\u0026family=Michroma\u0026family=Iceland\u0026family=ZCOOL+QingKe+HuangYou\u0026family=Zen+Tokyo+Zoo\u0026family=Major+Mono+Display\u0026family=Nova+Square\u0026family=Kelly+Slab\u0026family=Graduate\u0026family=Unica+One\u0026family=Aldrich\u0026family=Share+Tech+Mono\u0026family=Silkscreen\u0026family=Rajdhani:wght@700\u0026family=Jura:wght@700\u0026family=Goldman\u0026family=Tourney:wght@700\u0026family=Saira+Stencil+One\u0026family=Syncopate\u0026family=Fira+Code:wght@700\u0026family=DotGothic16\u0026display=swap\" rel=\"stylesheet\"\u003e\n  \u003cstyle\u003e\n    :root {\n      --primary-gradient: linear-gradient(\n        45deg, \n        #FF1493,   /* Deep Pink */\n        #FF69B4,   /* Hot Pink */\n        #FF00FF,   /* Magenta */\n        #FF4500,   /* Orange Red */\n        #8A2BE2    /* Blue Violet */\n      );\n      --cyberpunk-gradient: linear-gradient(\n        45deg,\n        #00FFFF, /* Cyan */\n        #FF00FF, /* Magenta */\n        #FFFF00  /* Yellow */\n      );\n      --sunset-gradient: linear-gradient(\n        45deg,\n        #FF7E5F, /* Coral */\n        #FEB47B, /* Peach */\n        #FF9966  /* Orange */\n      );\n      --ocean-gradient: linear-gradient(\n        45deg,\n        #2E3192, /* Deep Blue */\n        #1BFFFF  /* Light Cyan */\n      );\n      --forest-gradient: linear-gradient(\n        45deg,\n        #134E5E, /* Deep Teal */\n        #71B280  /* Light Green */\n      );\n      --rainbow-gradient: linear-gradient(\n        45deg,\n        #FF0000, /* Red */\n        #FF7F00, /* Orange */\n        #FFFF00, /* Yellow */\n        #00FF00, /* Green */\n        #0000FF, /* Blue */\n        #4B0082, /* Indigo */\n        #9400D3  /* Violet */\n      );\n    }\n    ..\n```\n\n**Full gist of the generated HTML / logic is at:\n[https://gist.github.com/jddunn/48bc03f3a9f85ffd8ccf90c801f6cf93](https://gist.github.com/jddunn/48bc03f3a9f85ffd8ccf90c801f6cf93)**\n\nWhile I don't have the original prompt that was used to create this file, the working version was generated in one-go (single round) with no prior context or examples of code given. In total, the prompt itself must've been a single paragraph long, and simply asked for a nicely designed and usable logo maker / generator that had export options and good styling options. Nothing was specified, and at the time, font management wasn't decided on a feature yet.\n\nUnfortunately, the original plan was to utilize Aider, one of the more widely supported (updated) and widely used libraries for generative AI and coding. Aider advertises itself as the AI pair programmer assistant. It feels like you may use vibe coding to use Aider, but the act of using Aider itself isn't necessarily vibe coding, nor is it an inherent act in any interaction with a LLM unless there's an intentional collaborative framework done by the user. In other words, vibe coding is applicable when it has to be a user that's testing the LLM's suggested coding changes and verifying the output. It is not the user asking the LLM for code which the user then goes through and rewrites to fit into their system / codebase.\n\nBut it's also tricky, because you can go \"in and out\" of vibe coding like state phases. One feature or class or function or even LOC can be \"vibe coded\", which simply should just imply the act of delegating more responsibility to the LLM to produce some working functionality than what the user assigns themselves. The dev becomes the pair programmer, instead of Aider, per se, being your pair programmer.\n\n![Aider interface](/assets/blog/this-is-aider.png)\n*Aider interface*\n\nThat said, the reason why vibe coding wasn't done through Aider simply had to do with the implementation of the newest upgrades of Aider itself. It was simply my / our team's personal experience that the far older versions of Aider provided much more usable functionality. We did make a solid attempt as Aider can edit and write files directly on the file system (as can some extensions in VS Code, Cursor, and other frameworks, but here we're just focusing on Aider as it seems like the current strongest contender), but after the first several edits we noticed functionality getting worse. But as we'll get into soon, this type of thing is by no means an issue exclusive to Aider and programming aide tools like it. It's an issue that naturally comes with the usability of all these LLMs, when we make the decision on interacting with them via an app, or via the website, or via the API, or via an agent API, etc., etc.\n\nSo we took the original lines of code we were given by Aider (what you saw above in the first iteration), and sent it to Claude Sonnet 3.7, and what was supposed to be a 2 hour project became a 2 day project which became a 10 day project.\n\n![Hello darkness my old friend](/assets/blog/logomaker-claude-horror-chat-history.png)\n*Hello darkness my old friend*\n\nThis is only the conversations list on Anthropic's Claude's UI (since this is the nicest looking and one with the most organized search). We used OpenAI's ChatGPT and Google Gemini's Pro paid plans, not just to test and compare, but because we had to. This thing still isn't done fully bug-free after 10 days!\n\nDon't want to add any new classes or fix any functions fully by hand, when we know darn well what needs to be fixed and what the LLM is continually doing and redoing wrong over and over again? That's not quite the vibe we're hoping to catch from the vibe coding experiment.\n\n## How to vibe with vibe coding vibes?\n\n![This type of prompt is not completely recommended but probably works well enough. Actually the curtness was intentional to see if Claude could extrapolate better functionality from just short instructions, which is how most casual users would try this, compared to something in-depth.](/assets/blog/logomaker-claude-demonstrates-coding-ability-1.png)\n*This type of prompt is not completely recommended but probably works well enough. Actually the curtness was intentional to see if Claude could extrapolate better functionality from just short instructions, which is how most casual users would try this, compared to something in-depth.*\n\nClaude generally always generates files in the right format, whether it's JavaScript, Python, Markdown, etc. Gemini does a great job with this too, though Anthropic's UI / UX far outclasses Gemini.\n\n![Claude's response showing code generation capabilities](/assets/blog/logomaker-claude-demonstrates-coding-ability-2.png)\n*Claude's response showing code generation capabilities*\n\nYou see we hit limits with Claude, of course, as we still desperately cling to the hope that we can just keep this constrained in one file, and be usable enough to be fun and decent. Plus, let's just see how far we can push these generations. Claude says we can just say \"continue\" and it'll work. Will it? (Hint: It didn't for OpenAI's GPT-4o models oftentimes, but Anthropic's UI is king as we've said).\n\n![Getting closer, but we're still not quite there yet..](/assets/blog/logomaker-claude-demonstrates-coding-ability-3.png)\n*Getting closer, but we're still not quite there yet..*\n\nAlright, let's just.. continue..\n\n![Getting closer, but we're still not quite there yet..](/assets/blog/logomaker-claude-demonstrates-coding-ability-4.png)\n*Getting closer, but we're still not quite there yet..*\n\nOkay, we started out with an 850 line file that actually gave us a fully functional app. Working PNG renders and working logos. This did prove my original theory and that I'm not completely delusional. I said to myself I've been wasting so much time in dead end dark patterns trying to find a free logo generator just to do some fun experimenting with, that it might be more efficient just to vibe code one and like ~magic~ it appears.. And to prove it, I got a fully working app (HTML with inline CSS / JS counts!) in 1-3 prompts from Aider using GPT-4o, that's incredibly limited and minimal sure, but truthfully did offer more functionality than the \"free\" demos these other sites were offering. And while that was written with Aider at first, the underlying LLM models are the same, and without a doubt (at the moment), in my experiences, OpenAI does a much superior job in responding to the user through the UI than giving the same prompt to the same model in Aider.\n\nAnd after asking Claude to simply improve it, we were left with almost double the LOC! But it doesn't compile because it's not finished, so we can't use it. And despite what Claude says in the UI, we are simply unable to continue any further, with this line of prompting (\"continue\"), to progress.\n\nWe know Claude and OpenAI can go into context windows of 100-200k, but apparently, that seems to only be in the Extended Mode. So what does this \"continue\" button even do? And what is this \"Extended Mode\", is this what we're forced into since the \"continue\" button doesn't work? Is it summarizing my conversation? Is it using Claude again to summarize my conversation? Is it aggregating the last 10 or so messages or however many until it reaches a predetermined limit (and how does it determine this limit, is it limiting my output window size thus limiting the ability for me to use Claude for pair programming?)?\n\nOutputs for LLMs are typically capped at 8,192 tokens, which is highly standard (as well as a completely arbitrarily defined number, one that can easily be extended by these respective API / LLM providers, and oftentimes is). The context windows are the same.\n\n## The real world, the real problems\n\nThe more relevant issue here is just how confusing and opaque these tools and the additional options being offered to us work. These are features that are actively costing us money. And time. So a lot of money actually (for a lot of working engineers). When I use ChatGPT-4.5, the more expensive model and take the time to move my conversational context and project info there, and accept the eventual price hikes that will come with these \"upgrades\" (surely, when they do..), I want to know what this is doing better, and maybe more importantly, why? Why is this model better, so I can actually make an informed decision on what to use? If I use X many calls from this model, am I limited then in my calls to other models? Oh no, I have to ask ChatGPT how much it costs to use ChatGPT?\n\n![I wasn't told I had hit my rate limit, or was coming anywhere near it during this conversation. Claude again has a better UX experience here as they warn you when you are beginning a long conversation that will quickly eat up your available credits](/assets/blog/chatgpt-o1-you-hit-rate-limit.png)\n*I wasn't told I had hit my rate limit, or was coming anywhere near it during this conversation. Claude again has a better UX experience here as they warn you when you are beginning a long conversation that will quickly eat up your available credits*\n\nHere's the distinction between a bug / error that's okay and one that's totally not. This is not a rate limit in the output of tokens generated, as in, it hit a limit in writing the script I asked for, had to stop, and would or could potentially continue finishing writing that script once my usage had renewed.\n\nNo, OpenAI's UI simply did not respond when I inputted my prompt to patiently await the LLM's response. Now this normally isn't an issue, and I swear I can remember ChatGPT's UI even used to say to just re-enter (nothin) in the chat window if no output was shown. But, when you are charging \"premium\" access for models, and heavily rate-limiting traffic to the point where every message has value, every few hours of waiting and refreshing of credits (in the case of Claude) is something to watch out for so you can continue utilizing these innovative tools, you can't simply not show a response, and not show why. You as the organization and provider should eat the cost and re-generate, even despite the fact that it damages the conversational flow, memory, and context window, because at least then you allow the user to continue on with developing without introducing roadblocks that are inherent to the tools that you are asking themselves to essentially marry themselves to as they get far enough along in development.\n\nThe nice thing about Google's UI with Gemini? Despite it being an absolute menace, resource hog that somehow is 10x slower on Chrome than Firefox for me, and an all-around eyesore, is when there's no response for an output, you can at least select an arrow button that shows the reasoning the LLM took to create that.. null or empty response. And that reasoning at least gives you a better understanding of what the LLM was \"thinking\" and oftentimes exactly what it was going to send to the user as its final output. It just like, chose not to?\n\n![By selecting the Show thinking button, you can see the exact reasoning the LLM is taking (note: it could be a series of calls as we have no transparency to what is happening in the web UI of Gemini) to answer. Which, oftentimes shows you the expected answer it was going to give but somehow didn't. Seeing as these issues are parallel across different LLM web UIs (OpenAI, Claude, and Gemini) through testing, the issue most likely seems inherent to LLM architecture and response mechanisms.](/assets/blog/google-gemini-pro-show-thinking-1.png)\n*By selecting the Show thinking button, you can see the exact reasoning the LLM is taking (note: it could be a series of calls as we have no transparency to what is happening in the web UI of Gemini) to answer. Which, oftentimes shows you the expected answer it was going to give but somehow didn't. Seeing as these issues are parallel across different LLM web UIs (OpenAI, Claude, and Gemini) through testing, the issue most likely seems inherent to LLM architecture and response mechanisms.*\n\n![Google Gemini Pro's thinking feature in action](/assets/blog/google-gemini-pro-show-thinking-2.png)\n*Google Gemini Pro's thinking feature in action*\n\nAnd for comparison's sake, ChatGPT's UI is by far the least consistent in terms of delivering consistent file formatting. ChatGPT finds it actually IMPOSSIBLE to deliver a single markdown file without messing up its formatting. Kidding, as it's likely just the devs behind this wilding entity messing up the building the UI empowering it to exist.\n\n![ChatGPT's inconsistent markdown formatting](/assets/blog/chatgpt-not-markdown.png)\n*ChatGPT's inconsistent markdown formatting*\n\nThat's not the file fully in markdown actually. Markdown should just literally look like a text file with special formatting characters.\n\n![More markdown formatting issues with ChatGPT](/assets/blog/chatgpt-not-markdown-2.png)\n*More markdown formatting issues with ChatGPT*\n\nThat's also, just partially markdown, not all markdown.\n\n![ChatGPT giving a \"full\" refactored file from a script that was originally 1500 lines of code in JS](/assets/blog/chatgpt-not-giving-full-file.png)\n*ChatGPT giving a \"full\" refactored file from a script that was originally 1500 lines of code in JS*\n\nThat's ChatGPT giving me a \"full\" refactored file from a script that was originally 1500 lines of code in JS (don't judge, this is an LLM-generated or \"vibe-coded\" project remember?). It refactored it into 200 lines. It refactored like losing weight by cutting a limb off. Claude runs into the same issues we've seen earlier with its \"continue\" limit, which genuinely seems to be a UI limitation, which is very unfortunate, as Sonnet 3.7 (at the moment) was doing great work up until it hit its limits. Gemini Pro 2.5? This was the only model capable of generating a full ~2000 LOC file coherently with minimal hallucinations in one go.\n\nHere is a hint with ChatGPT, our OG LLM provider. If it asks you \"Would you like this answered in chat?\", instead of it writing in a file in a text editor inside the chat window (which is what would be happening here), you click on that thing as soon as you're able to before this other terrifying UI feature starts controlling your conversation and showing files rendered in the most unclear way.\n\nThough, I must emphasize, at the moment, as with anything with these APIs and providers, it seems everything is always subject to change, sometimes even at the whim of competitors:\n- [Google really wants to punish OpenAI for that one](https://www.reddit.com/r/Bard/comments/1idmqul/google_really_wants_to_punish_openai_for_that_one/)\n- [OpenAI plans to announce Google search competitor](https://www.reddit.com/r/technology/comments/1co9lcg/openai_plans_to_announce_google_search_competitor/)\n- [Google faked the release date for the updates](https://www.reddit.com/r/OpenAI/comments/1e8mfmx/google_faked_the_release_date_for_the_updates_and/)\n\nSomehow this transparency of showing thinking / reasoning from Gemini Pro also demonstrates the fundamental lack of transparency these platforms by design invite. Why show me the thought process if I don't understand how that thinking works? Is it just like, 4 API calls on top of each other? Does that mean it uses 4x as many \"credits\" as I would have in my plan then?\n\nDevs behind these providers may just try new features or A/B experiments, and you might not have any idea about a change until it starts to go trending on Reddit, Twitter, etc.\n- [Was GPT-4o nerfed again?](https://www.reddit.com/r/OpenAI/comments/1jlwhs0/was_gpt4o_nerfed_again/)\n- [Boys what OpenAI did to this model](https://www.reddit.com/r/singularity/comments/1gy7p1d/boys_what_openai_did_to_this_model/)\n- [OpenAI nerfing GPT feels like a major downgrade](https://www.reddit.com/r/ChatGPT/comments/1iu237v/openai_nerfing_gpt_feels_like_a_major_downgrade/)\n- [Hacker News discussion on nerfing](https://news.ycombinator.com/item?id=40077683)\n- [Claude 3.7 Max been nerfed?](https://forum.cursor.com/t/claude-3-7-max-been-nerfed/73840)\n- [Whenever people say X model has been nerfed it's almost always complete bulls**t](https://www.threads.net/@sobri909/post/DH-P4irxjrU/yeah-whenever-people-say-x-model-has-been-nerfed-its-almost-aways-complete-bulls)\n- [Hacker News item 41327360](https://news.ycombinator.com/item?id=41327360)\n- [Twitter discussion on model changes](https://x.com/samim/status/1876005616403300582)\n\nIt's clear the user community around AI has a lot of fears that don't involve becoming obsolete by singularity or automation. Users are heavily embracing generative AI, at an almost alarming rate.\n\n![Scene from the movie \"Her\" by Warner Bros](/assets/blog/her-movie-screenshot-warner-bros.png)\n*Scene from the movie \"Her\" by Warner Bros*\n\nAnd we're still left in the dark in other ways. How much of a competitive edge do companies like OpenAI and Anthropic get when they can internally adjust the outputs and filters / censors of their models at will? How much access is available for large organizations (governments, banks, hedge funds, or huge tech companies with their own silos like Oracle, Microsoft) to \"buy\" control, even temporary or one-time arrangements, over these inputs and outputs that are completely black-box to your average user?\n\n![The live site of Logomaker, at https://manicinc.github.io/logomaker, which will live here free forever so long as GitHub Pages is free.](/assets/blog/logomaker-live-site.png)\n*The live site of Logomaker, at https://manicinc.github.io/logomaker, which will live here free forever so long as GitHub Pages is free.*\n\n![Logomaker live site 2](/assets/blog/logomaker-live-site-2.png)\n\nThis app was built as an experimental work to test the current capabilities of different LLMs as well as their providers and the accompanying UI features serviced by them. It's meant as a fun, useful, and chaotic work where the dev was fully dedicated to just using vibe coding, or allowing the LLM to generate code and functions with detailed and clear technical guidance. Individual small fixes for issues from the LLM providers (the code given out in the UIs) were fixed sometimes with GitHub Co-Pilot for convenience's (and budget) sake.\n\nOriginally, the hope was to get this whole thing done in just 1 HTML file! And not take so many multiple days (working on and off) to finish up. And it was, in just a few prompts too. We'll post that as a snippet here: [coming-soon](coming-soon). But, it just seemed like every new feature was just a quick prompt or two away, and so on, and so on, and..\n\nSo one day, we had an intelligent [font management system](#) that could lazily load gigabytes of fonts in a speedy way, a [build setup](#) that worked with our other [portapack-package](#) and could [compile](#) into an Electron app, a live dev workflow.\n\nThen the next day, we had full SVG support. And not just support for static SVGs, but actual animations! Something really difficult to pull off in imaging applications, and something I had never even thought about (or had any idea on how to implement, as all those algorithmic and style building / XML techniques were done by the LLM, with no external sources / documentation given for reference).\n\nAnd so on and on with each passing day, until a 2 hour project became a 2 day project which became a 10 day project which is the full-fledged design playground application you see linked below.\n\nWe kept things simple, static assets, all client-side rendering, no server required to run the app (not necessarily, anyway, as we have a multitude of building / running options), and, just JavaScript. No TypeScript, unit tests, or refactoring that wasn't done on-the-fly. This wasn't a scientific experiment, but given every function was definitively written by an LLM (by intention), it's safe to say well over 90% of the codebase was done by generative AI. At least 80% of the docs you see in the repo were written by generative AI. And 0% of this article was written by generative AI.\n\nWhat do you think about the source code, designs, and end results that these large language models, these AI assistants, these slaves workers collaborators agents did?\n\n- [Live Demo: https://manicinc.github.io/logomaker](https://manicinc.github.io/logomaker)\n- [GitHub Repo: https://github.com/manicinc/logomaker](https://github.com/manicinc/logomaker)"])</script><script>self.__next_f.push([1,"8:[[\"$\",\"div\",null,{\"className\":\"bg-[#1a1a1e]\",\"children\":[\"$\",\"$L11\",null,{}]}],[\"$\",\"main\",null,{\"className\":\"blog-container\",\"children\":[[\"$\",\"article\",null,{\"className\":\"blog-post\",\"children\":[null,[\"$\",\"h1\",null,{\"className\":\"blog-title\",\"children\":\"Logomaker: An experiment in human-computer interaction and ✨vibe coding ✨\"}],[\"$\",\"div\",null,{\"className\":\"blog-meta-container\",\"children\":[[\"$\",\"p\",null,{\"className\":\"blog-meta\",\"children\":[[\"$\",\"span\",null,{\"children\":[\"Johnny Dunn\",\" • \"]}],[\"$\",\"span\",null,{\"className\":\"blog-date-published\",\"title\":\"Published: 2025-04-08\",\"children\":[[\"$\",\"svg\",null,{\"width\":\"14\",\"height\":\"14\",\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":\"2\",\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"children\":[[\"$\",\"rect\",null,{\"x\":\"3\",\"y\":\"4\",\"width\":\"18\",\"height\":\"18\",\"rx\":\"2\",\"ry\":\"2\"}],[\"$\",\"line\",null,{\"x1\":\"16\",\"y1\":\"2\",\"x2\":\"16\",\"y2\":\"6\"}],[\"$\",\"line\",null,{\"x1\":\"8\",\"y1\":\"2\",\"x2\":\"8\",\"y2\":\"6\"}],[\"$\",\"line\",null,{\"x1\":\"3\",\"y1\":\"10\",\"x2\":\"21\",\"y2\":\"10\"}]]}],[\"$\",\"time\",null,{\"dateTime\":\"2025-04-08\",\"children\":\"April 8, 2025\"}]]}],[\"$\",\"span\",null,{\"className\":\"reading-time\",\"children\":[[\"$\",\"svg\",null,{\"width\":\"14\",\"height\":\"14\",\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":\"2\",\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"children\":[[\"$\",\"circle\",null,{\"cx\":\"12\",\"cy\":\"12\",\"r\":\"10\"}],[\"$\",\"polyline\",null,{\"points\":\"12 6 12 12 16 14\"}]]}],24,\" min read\"]}]]}],[\"$\",\"$L12\",null,{\"href\":\"/category/thinkpieces\",\"className\":\"blog-category-link\",\"children\":[\"Filed under: \",[\"$\",\"span\",null,{\"className\":\"category-name\",\"children\":\"THINKPIECES\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"blog-content\",\"children\":[\"$\",\"$L13\",null,{\"children\":\"$14\"}]}],[\"$\",\"div\",null,{\"className\":\"blog-tags\",\"children\":[[\"$\",\"$L12\",\"featured\",{\"href\":\"/tags/featured\",\"className\":\"blog-tag\",\"children\":[\"#\",\"featured\"]}]]}],\"$undefined\"]}],[\"$\",\"div\",null,{\"className\":\"blog-navigation\",\"children\":[\"$\",\"$L12\",null,{\"href\":\"/blog\",\"className\":\"back-to-blog\",\"children\":\"← Back to all posts\"}]}]]}]]\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\na:null\n"])</script><script>self.__next_f.push([1,"b:null\nf:[[\"$\",\"title\",\"0\",{\"children\":\"Logomaker: An experiment in human-computer interaction and ✨vibe coding ✨\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Exploring the world of vibe coding through a logo creation tool, with insights on different LLMs and the changing landscape of software development.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Johnny Dunn\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:title\",\"content\":\"Logomaker: An experiment in human-computer interaction and ✨vibe coding ✨\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:description\",\"content\":\"Exploring the world of vibe coding through a logo creation tool, with insights on different LLMs and the changing landscape of software development.\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"6\",{\"property\":\"article:published_time\",\"content\":\"2025-04-08\"}],[\"$\",\"meta\",\"7\",{\"property\":\"article:author\",\"content\":\"Johnny Dunn\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:title\",\"content\":\"Logomaker: An experiment in human-computer interaction and ✨vibe coding ✨\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:description\",\"content\":\"Exploring the world of vibe coding through a logo creation tool, with insights on different LLMs and the changing landscape of software development.\"}]]\n"])</script></body></html>